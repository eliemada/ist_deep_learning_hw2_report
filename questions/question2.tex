\section{Question 2: Image Classification with CNNs}

\subsection{2.1 Basic CNN Implementation}

We implemented a convolutional neural network for the Intel Image Classification dataset with three convolutional blocks followed by an MLP classifier. The results of our training with different learning rates are shown in \cref{fig:training_loss_comparison,fig:validation_acc_comparison}. The initial architecture consisted of:

\begin{itemize}
    \item Three convolutional blocks with output channels (32, 64, 128)
    \item Each block containing:
        \begin{itemize}
            \item 3x3 convolution with stride 1 and padding 1
            \item ReLU activation
            \item 2x2 max pooling with stride 2
            \item Dropout (p = 0.1)
        \end{itemize}
    \item MLP block with:
        \begin{itemize}
            \item Flattened convolutional output
            \item Dense layer (1024 units)
            \item ReLU activation
            \item Dropout (p = 0.1)
            \item Dense layer (512 units)
            \item ReLU activation
            \item Output layer (6 units) with LogSoftmax
        \end{itemize}
\end{itemize}

As shown in \cref{fig:training_loss_comparison,fig:validation_acc_comparison}, after testing different learning rates (0.1, 0.01, 0.001), we found that lr = 0.01 provided the best performance. This configuration achieved a test accuracy of 70.33\% with 5,340,742 trainable parameters. The model demonstrated stable learning behavior, though there was room for improvement in terms of both accuracy and efficiency.

\subsection{2.2 Enhanced CNN with Batch Normalization}
We then modified the architecture to include batch normalization and global average pooling, with results shown in \cref{fig:metrics_with_batchnorm}:

\begin{itemize}
    \item Added batch normalization after each convolution layer
    \item Replaced flattening with global average pooling (1x1 kernel)
    \item Added batch normalization in the MLP block before dropout
\end{itemize}

This enhanced version showed significant improvements, as evident in \cref{fig:metrics_with_batchnorm}:
\begin{itemize}
    \item Increased test accuracy to 75.23\% (+4.9\% absolute improvement)
    \item Reduced model parameters to 756,742 (85.8\% reduction)
    \item Better training stability with:
        \begin{itemize}
            \item Training loss: 0.5421
            \item Validation loss: 0.6795
            \item Validation accuracy: 75.43\%
        \end{itemize}
\end{itemize}

\subsection{Analysis of Improvements}

The significant improvements can be attributed to several factors, which are reflected in the training metrics shown in \cref{fig:metrics_with_batchnorm}:

\subsubsection{1. Batch Normalization Effects}
The addition of batch normalization layers provided several benefits:
\begin{itemize}
    \item Reduced internal covariate shift
    \item Enabled faster training with more stable gradients
    \item Acted as a regularizer, improving generalization
    \item Allowed for better gradient flow through the network
\end{itemize}

\subsubsection{2. Global Average Pooling}
Replacing the flattening operation with global average pooling:
\begin{itemize}
    \item Drastically reduced the number of parameters (from 5.3M to 756K)
    \item Enforced stronger spatial feature learning
    \item Improved generalization by reducing overfitting
    \item Made the model more robust to spatial translations
\end{itemize}

\subsection{2.3 Analysis of Parameter Count Differences}

We implemented the \texttt{get\_number\_trainable\_params} function to analyze the number of trainable parameters in both networks. The significant difference in parameter count (5,340,742 vs 756,742) can be attributed to several key factors:

\begin{enumerate}
    \item \textbf{Flattening vs Global Average Pooling}: The most substantial reduction comes from replacing the flattening operation with global average pooling:
    \begin{itemize}
        \item In the original network, flattening created a large feature vector (128 * width * height) that connected to the first dense layer (1024 units)
        \item Global average pooling reduces each feature map to a single value, resulting in just 128 features connecting to the dense layer
        \item This change alone eliminated millions of parameters in the first dense layer connections
    \end{itemize}
    
    \item \textbf{Preserved Information Flow}: Despite the massive reduction in parameters, the network maintains (and even improves) its performance because:
    \begin{itemize}
        \item Global average pooling maintains the spatial relationship of features
        \item Batch normalization helps in better gradient flow and feature normalization
        \item The reduced parameter count acts as a form of regularization, preventing overfitting
    \end{itemize}
\end{enumerate}

The improved performance with fewer parameters demonstrates that the original architecture was overparameterized, and the modifications helped create a more efficient and effective model.

\subsection{2.4 Kernel Size and Pooling Effects}

The choice of small kernels (3x3) instead of larger kernels (width × height) and the use of pooling layers are fundamental design decisions in modern CNNs:

\subsubsection{Small Kernel Benefits}
\begin{itemize}
    \item \textbf{Parameter Efficiency}: Small kernels require significantly fewer parameters. A 3x3 kernel needs only 9 parameters per channel, while a full width×height kernel would need width×height parameters.
    
    \item \textbf{Hierarchical Feature Learning}: Stacking multiple layers with small kernels allows the network to:
    \begin{itemize}
        \item Build increasingly complex features layer by layer
        \item Capture the same receptive field as larger kernels with fewer parameters
        \item Learn more discriminative features through multiple non-linear transformations
    \end{itemize}
    
    \item \textbf{Better Generalization}: Small kernels force the network to learn local patterns first before combining them into more complex features, leading to better generalization.
\end{itemize}

\subsubsection{Pooling Layer Effects}
The pooling layers (2x2 max pooling in our implementation) serve several crucial purposes:

\begin{itemize}
    \item \textbf{Dimensionality Reduction}: Each pooling layer reduces spatial dimensions by half, making computation more efficient.
    
    \item \textbf{Translation Invariance}: Max pooling helps the network become more robust to small translations in the input, as it preserves the strongest feature activations in each region.
    
    \item \textbf{Hierarchical Processing}: Combined with the convolution layers, pooling helps create a hierarchy where:
    \begin{itemize}
        \item Early layers detect fine details (edges, textures)
        \item Middle layers combine these into parts
        \item Later layers recognize higher-level concepts
    \end{itemize}
    
    \item \textbf{Increased Receptive Field}: Each neuron in deeper layers can "see" a larger portion of the input image, allowing the network to capture more context while maintaining parameter efficiency.
\end{itemize}

This combination of small kernels and pooling layers creates a powerful architecture that can efficiently learn hierarchical representations while maintaining computational feasibility and good generalization properties.

\subsection{Future Considerations}

Potential areas for further improvement could include:
\begin{itemize}
    \item Experimenting with different learning rate schedules
    \item Investigating other normalization techniques (e.g., Layer Normalization)
    \item Testing different dropout rates
\end{itemize}

\subsection{Conclusion}

The enhanced CNN architecture demonstrated substantial improvements over the baseline model in multiple aspects, as evidenced by the comparison between \cref{fig:training_loss_comparison,fig:validation_acc_comparison} and \cref{fig:metrics_with_batchnorm}:

1. \textbf{Efficiency}: The 85.8\% reduction in parameters while improving accuracy suggests a more efficient use of model capacity.

2. \textbf{Performance}: The 4.9\% absolute improvement in test accuracy indicates better generalization.

3. \textbf{Training Stability}: The combination of batch normalization and global average pooling led to more stable training dynamics.

These results highlight the importance of modern architectural choices in CNN design, particularly the synergistic effects of batch normalization and global average pooling. The improvements came not from adding complexity, but from making the architecture more efficient and trainable.